{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630c5947",
   "metadata": {},
   "source": [
    "adamW\n",
    "convolution\n",
    "residual\n",
    "bath normalization, dropout\n",
    "data augmentation\n",
    "    elastic deform\n",
    "    gaussian noise\n",
    "    rotation\n",
    "    crop\n",
    "    dynamic zoom\n",
    "    blur\n",
    "    가리기\n",
    "    3d rotation\n",
    "    +여러가지 mnist data\n",
    "    \n",
    "learning late sceduling#to learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9ee2d",
   "metadata": {},
   "source": [
    "구조\n",
    "    eval() mod 일 때에는 기존 convolutional network+positional embedding 처럼 동작.\n",
    "    즉, 4채널짜리 Convolution이 되는 것.\n",
    "    그러나 train mod 일 때에는, 사진이라고 하는 데이터의 특성을 극한으로 활용\n",
    "    Pre-augmented dataset(digit-five 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34b91d",
   "metadata": {},
   "source": [
    "구조 자세히\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529084f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data 관련 임포트\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from scipy import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036135ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#유틸 임포트\n",
    "import pprint\n",
    "from os.path import exists\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a8ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FOLDER = '../datasets/DIGIT_FIVE/'\n",
    "files = ['mnistm_with_label.mat','svhn_train_32x32.mat','syn_number.mat','usps_28x28.mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc72634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistAm(Dataset):\n",
    "    def __init__(self, train = True):\n",
    "        #transforms.Compose([transforms.ToTensor(),lambda x: x.repeat(1,3,1,1), lambda x: torch.mul(x,torch.rand(3,28,28))])\n",
    "        self.train = train\n",
    "        \n",
    "        mat_file = io.loadmat(PATH_FOLDER+files[3])\n",
    "        if self.train == True:\n",
    "            mnist = datasets.MNIST('./datasets',train=True , download = True)\n",
    "            data1 = mnist.train_data.unsqueeze(1)\n",
    "            data2 = torch.tensor(mat_file['dataset'][0][0],dtype=torch.float32)*255\n",
    "            self.data = torch.cat((data1, data2), dim=0)\n",
    "            label1 = mnist.train_labels\n",
    "            label2 = torch.tensor(mat_file['dataset'][0][1].reshape(-1))#vector\n",
    "            self.label = torch.cat((label1, label2), dim=0)\n",
    "        else:\n",
    "            mnist = datasets.MNIST('./datasets',train=False , download = True)\n",
    "            data1 = mnist.test_data.unsqueeze(1)\n",
    "            data2 = torch.tensor(mat_file['dataset'][1][0],dtype=torch.float32)*255\n",
    "            self.data = torch.cat((data1, data2), dim=0)\n",
    "            label1 = mnist.test_labels\n",
    "            label2 = torch.tensor(mat_file['dataset'][1][1].reshape(-1))#vector\n",
    "            self.label = torch.cat((label1, label2), dim=0)\n",
    "        self.data = self.data.repeat(1,3,1,1)/255\n",
    "        self.rtransform = transforms.RandomApply(transforms = [lambda x: torch.mul(x,torch.rand(3,28,28))], p=1)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.rtransform(self.data[index]) , self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f68f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistm(Dataset):\n",
    "    def __init__(self, train = True):\n",
    "        self.train = train\n",
    "        mat_file = io.loadmat(PATH_FOLDER+files[0])\n",
    "        if self.train == True:\n",
    "            self.data = torch.tensor(mat_file['train'].transpose(0,3,1,2)/255,dtype=torch.float32)\n",
    "            self.label = torch.tensor(mat_file['label_train'].argmax(axis = 1))\n",
    "        else:\n",
    "            self.data = torch.tensor(mat_file['test'].transpose(0,3,1,2)/255,dtype=torch.float32)\n",
    "            self.label = torch.tensor(mat_file['label_test'].argmax(axis = 1))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91722e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class svhn(Dataset):\n",
    "    def __init__(self, train = True):\n",
    "        tn = 60000#max:73257\n",
    "        self.train = train\n",
    "        mat_file = io.loadmat(PATH_FOLDER+files[1])\n",
    "        if self.train == True:\n",
    "            self.data = torch.tensor(mat_file['X'].transpose(3,2,0,1)[:tn]/255,dtype=torch.float32)\n",
    "            self.label = torch.tensor(mat_file['y'][:tn].reshape(-1)-1)\n",
    "        else:\n",
    "            self.data = torch.tensor(mat_file['X'].transpose(3,2,0,1)[tn:]/255,dtype=torch.float32)\n",
    "            self.label = torch.tensor(mat_file['y'][tn:].reshape(-1)-1)\n",
    "        resize = transforms.Resize((28,28))\n",
    "        self.data = resize(self.data)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549b2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class svn(Dataset):\n",
    "    def __init__(self, train = True):\n",
    "        self.train = train\n",
    "        mat_file = io.loadmat(PATH_FOLDER+files[2])\n",
    "        if self.train == True:\n",
    "            self.data = torch.tensor(mat_file['train_data'].transpose(0,3,1,2)/255,dtype=torch.float32)\n",
    "            self.label = torch.tensor(mat_file['train_label'].reshape(-1))\n",
    "        else:\n",
    "            self.data = torch.tensor(mat_file['test_data'].transpose(0,3,1,2)/255,dtype=torch.float32)\n",
    "            self.label = torch.tensor(mat_file['test_label'].reshape(-1))\n",
    "        resize = transforms.Resize((28,28))\n",
    "        self.data = resize(self.data)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20cd3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class comdata(Dataset):\n",
    "    def __init__(self, train = True):\n",
    "        self.datasets = [\n",
    "            mnistAm(train=train),\n",
    "            mnistm(train=train),\n",
    "            svhn(train=train),\n",
    "            svn(train=train)\n",
    "        ]\n",
    "        self.lengthsegment = []\n",
    "        for dataset in self.datasets:\n",
    "            if len(self.lengthsegment) == 0:\n",
    "                last_len = 0\n",
    "            else:\n",
    "                last_len = self.lengthsegment[-1]\n",
    "            self.lengthsegment.append(len(dataset)+last_len)\n",
    "        self.lengthsegment = torch.tensor(self.lengthsegment)\n",
    "    def __len__(self):\n",
    "        return self.lengthsegment[-1]\n",
    "    def __getitem__(self, index):\n",
    "        cansize = self.lengthsegment[self.lengthsegment >= index+1][0]\n",
    "        canindex = (self.lengthsegment == cansize).int().argmax()\n",
    "        return self.datasets[canindex][-(cansize-(index+1)+1)]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b32788aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사진 뒤집고, 노이즈 적용하고,회전시키고, random perspective.\n",
    "t_noise = transforms.Compose([lambda x: x+torch.randn(x.size()).to(device=device)*np.random.randint(5)*0.01, lambda x: torch.clamp(x, min=0, max=1) ])\n",
    "ut = transforms.Compose([\n",
    "    transforms.RandomInvert(),\n",
    "    t_noise,\n",
    "    transforms.RandomRotation(degrees = (-90,90)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p = 0.5)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9557ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntdf = comdata(train=False)\\n\\n\\n\\nt = torch.tensor([-2,3,1])\\n\\ntorch.clamp(t,min=-1)\\n\\nt_noise = transforms.Compose([lambda x: x+torch.randn(x.size())*np.random.randint(5)*0.01, lambda x: torch.clamp(x, min=0, max=1) ])\\nut = transforms.Compose([\\n    transforms.RandomInvert(),\\n    t_noise,\\n    transforms.RandomRotation(degrees = (-90,90)),\\n    transforms.RandomPerspective(distortion_scale=0.3, p = 0.5)\\n])\\n\\nn = 23043\\nplt.imshow(ut(tdf[n][0]).permute(1,2,0))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tdf = comdata(train=False)\n",
    "\n",
    "\n",
    "\n",
    "t = torch.tensor([-2,3,1])\n",
    "\n",
    "torch.clamp(t,min=-1)\n",
    "\n",
    "t_noise = transforms.Compose([lambda x: x+torch.randn(x.size())*np.random.randint(5)*0.01, lambda x: torch.clamp(x, min=0, max=1) ])\n",
    "ut = transforms.Compose([\n",
    "    transforms.RandomInvert(),\n",
    "    t_noise,\n",
    "    transforms.RandomRotation(degrees = (-90,90)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p = 0.5)\n",
    "])\n",
    "\n",
    "n = 23043\n",
    "plt.imshow(ut(tdf[n][0]).permute(1,2,0))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f8d93a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DEVICE 0: NVIDIA GeForce RTX 3090\n",
      "- Memory Usage:\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.0 GB\n",
      "\n",
      "# DEVICE 1: NVIDIA GeForce RTX 3090\n",
      "- Memory Usage:\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.0 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Conda1/lib/python3.9/site-packages/torch/cuda/memory.py:384: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def GPUreport():\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"# DEVICE {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(\"- Memory Usage:\")\n",
    "            print(f\"  Allocated: {round(torch.cuda.memory_allocated(i)/1024**3,1)} GB\")\n",
    "            print(f\"  Cached:    {round(torch.cuda.memory_cached(i)/1024**3,1)} GB\\n\")\n",
    "    else:\n",
    "        print(\"# GPU is not available\")\n",
    "GPUreport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8951ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device is:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 1#\n",
    "\n",
    "device = (torch.device(f'cuda:{GPU_NUM}') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "torch.cuda.set_device(device)\n",
    "#device = torch.device('cpu')#cpu\n",
    "\n",
    "print(\"Current Device is: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3800fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_cha, out_cha, stride = 1):\n",
    "    #downsampling 할때 쓰는 conv\n",
    "    return nn.Conv2d(in_cha, out_cha, kernel_size=1, stride=stride, bias=False).to(device = device)\n",
    "\n",
    "class res_block(nn.Module):#shortcut 없는 그냥 Block\n",
    "    #positional_dimension = 3\n",
    "    def __init__(self, dropp, out_cha, stride=1, dilation=1):# Input channel 에 adaptive 하게 작동.\n",
    "        self.outlist = []#debug\n",
    "        self.pd = 6 #positional_dimension\n",
    "        super(res_block, self).__init__()\n",
    "        self.param_dict = nn.ParameterDict()\n",
    "        self.hyper = {'out_cha':out_cha, 'stride':stride, 'dilation':dilation}\n",
    "        self.downsample = None\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.posi1 = None#positional embedding\n",
    "        self.conv1 = None\n",
    "        self.dropout1 = nn.Dropout(dropp)\n",
    "        self.bn1 = nn.BatchNorm2d(out_cha).to(device = device)\n",
    "        \n",
    "        self.posi2 = None#positional embedding\n",
    "        self.conv2 = nn.Conv2d(self.hyper['out_cha']+self.pd , self.hyper['out_cha'], kernel_size = 3, stride = 1, padding = self.hyper['dilation'], dilation=self.hyper['dilation']).to(device = device)\n",
    "        self.dropout2 = nn.Dropout(dropp)\n",
    "        self.bn2 = nn.BatchNorm2d(out_cha).to(device = device)\n",
    "    def backup(self, tensor):\n",
    "        self.outlist.append(tensor)\n",
    "    def clearbackup(self):\n",
    "        self.outlist = []\n",
    "    def forward(self, x):#x=N,C,H,W\n",
    "        identity = x\n",
    "        batch_size = x.shape[0]\n",
    "        if self.posi1 is None:\n",
    "            square_h_1 = x.shape[2]#Input 사진이 정사각형임을 가정.\n",
    "            self.posi1 = nn.Parameter(torch.randn(self.pd, square_h_1, square_h_1).unsqueeze(0).to(device = device), requires_grad = True)\n",
    "            #self.param_dict[\"posi_embed1\"] = self.posi1\n",
    "            #self.posi1 = self.posi1.to(device = device)\n",
    "        duplicated_1 = self.posi1.repeat(batch_size,1,1,1)#입력의 batch size만큼 반복\n",
    "        out = torch.cat((x,duplicated_1), dim = 1)\n",
    "        if self.conv1 is None:\n",
    "            self.conv1 = nn.Conv2d(x.shape[1]+self.pd , self.hyper['out_cha'], kernel_size = 3, stride = self.hyper['stride'], padding=self.hyper['dilation'], dilation=self.hyper['dilation']).to(device = device)\n",
    "            \n",
    "        out = self.conv1(out)\n",
    "        #self.backup(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.bn1(out)\n",
    "        #self.backup(out)\n",
    "        out = self.relu(out)\n",
    "        #self.backup(out)\n",
    "        #layer 2\n",
    "        if self.posi2 is None:\n",
    "            if self.hyper['stride'] == 1:\n",
    "                self.posi2 = self.posi1#input-output shape를 그대로 쓰면 됨\n",
    "            else:\n",
    "                square_h_2 = out.shape[2]\n",
    "                self.posi2 = nn.Parameter(torch.randn(self.pd,square_h_2, square_h_2).unsqueeze(0).to(device = device), requires_grad = True)\n",
    "            #self.param_dict[\"posi_embed2\"] = self.posi2\n",
    "            #self.posi2 = self.posi2.to(device = device)\n",
    "        duplicated_2 = self.posi2.repeat(batch_size,1,1,1)\n",
    "        out = torch.cat((out,duplicated_2), dim = 1)\n",
    "        out = self.conv2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample == None:\n",
    "            if self.hyper['stride'] > 1 or x.shape[1] != out.shape[1]: #downsample 이거나 또는 channel size가 일치하지 않을 시\n",
    "                self.downsample = conv1x1(identity.shape[1], self.hyper['out_cha'], stride = self.hyper['stride'])\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d9e7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class adaptiveLinear(nn.Module):\n",
    "    def __init__(self,output_size):\n",
    "        super(adaptiveLinear, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.linear = None\n",
    "    def forward(self,x):#x:batch size ~~blabla\n",
    "        data = x.view(x.shape[0], -1)\n",
    "        if self.linear is None:\n",
    "            self.linear = nn.Linear(data.shape[1],self.output_size).to(device = device)\n",
    "            #self.generated = None\n",
    "        return self.linear(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "618def78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class posiresNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(posiresNN, self).__init__()\n",
    "        self.modelname = \"posires\"\n",
    "        self.residual = nn.Sequential(\n",
    "            res_block(0, 64),\n",
    "            res_block(0, 64),\n",
    "            res_block(0, 64),\n",
    "            res_block(0, 64),\n",
    "            res_block(0, 64),\n",
    "            res_block(0, 128, stride=2),#14*14\n",
    "            res_block(0, 128),\n",
    "            adaptiveLinear(10))\n",
    "    def forward(self, x):\n",
    "        return self.residual(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89a252d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(optimizer, model, loss_fn, train_loader, test_loader, epochs, train_loss_list, val_loss_list, accuracy_list):\n",
    "    AGUMENT = 1\n",
    "    for epoch in range(1,epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        #model specific code\n",
    "        if epoch == 1:\n",
    "            print('epoch 1')\n",
    "            previous = model.residual[0].posi1[0][0].detach().to(device='cpu').numpy()\n",
    "        current = model.residual[0].posi1[0][0].detach().to(device='cpu').numpy()\n",
    "        plt.imshow(current-previous, cmap='gray')\n",
    "        plt.show()\n",
    "        previous = model.residual[0].posi1[0][0].detach().to(device='cpu').numpy()\n",
    "        #model specific code\n",
    "        batchiter = 0#batchiter: 배치의 개수. batchiter*batch_size 해야 대략적인 총 데이터 개수.\n",
    "        for images, labels, in train_loader:\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = None\n",
    "            labels_a = None\n",
    "            for i in range(AGUMENT):\n",
    "                images_a = ut(images.to(device=device))\n",
    "                if outputs is not None:\n",
    "                    outputs = torch.cat((outputs, model(images_a)),dim=0)#batchsize*10\n",
    "                    labels_a = torch.cat((labels_a,labels),dim=0)\n",
    "                else:\n",
    "                    outputs = model(images_a)\n",
    "                    labels_a = labels\n",
    "            loss = loss_fn(outputs, labels_a)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        loss_train = loss_train/len(train_loader)\n",
    "        loss_validation, accuracy = evaluate_model(model, test_loader, loss_fn)\n",
    "        train_loss_list += [loss_train]\n",
    "        val_loss_list += [loss_validation]\n",
    "        accuracy_list += [accuracy]\n",
    "        torch.save(model.state_dict(), model.modelname+'.pt')#debug\n",
    "        print(\"epoch:\",epoch, \" train_loss: \", loss_train, \" val_loss: \", loss_validation, \" val_accuracy: \", accuracy)\n",
    "        \n",
    "\n",
    "        \n",
    "def evaluate_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_size = 0\n",
    "    loss_test = 0.0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, in data_loader:\n",
    "            images = images.to(device=device)#batchsize*1*28*28\n",
    "            labels = labels.to(device=device)#batchsize\n",
    "            outputs = model(images)#batchsize*10\n",
    "            output_labels = outputs.argmax(dim=1)\n",
    "            accuracy += torch.sum(output_labels == labels).item()\n",
    "            total_size += len(labels)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss_test += loss.item()\n",
    "    loss_test = loss_test/len(data_loader)\n",
    "    accuracy = accuracy/total_size\n",
    "    model.train()\n",
    "    return loss_test, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f022289",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Conda1/lib/python3.9/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/opt/conda/envs/Conda1/lib/python3.9/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/opt/conda/envs/Conda1/lib/python3.9/site-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/opt/conda/envs/Conda1/lib/python3.9/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "BATCHSIZE_TRAIN = 2000\n",
    "DROPOUT_PERCENT = 0\n",
    "LR = 4e-3\n",
    "EPOCHES = 0\n",
    "train_loader = DataLoader(comdata(train=True), batch_size = BATCHSIZE_TRAIN, shuffle = True) #20만개 넘음\n",
    "test_loader = DataLoader(comdata(train=False), batch_size = 2000, shuffle =True) #7만개?\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3229cd98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posiresNN(\n",
      "  (residual): Sequential(\n",
      "    (0): res_block(\n",
      "      (param_dict): ParameterDict()\n",
      "      (relu): ReLU()\n",
      "      (dropout1): Dropout(p=0, inplace=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(70, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout2): Dropout(p=0, inplace=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): res_block(\n",
      "      (param_dict): ParameterDict()\n",
      "      (relu): ReLU()\n",
      "      (dropout1): Dropout(p=0, inplace=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(70, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout2): Dropout(p=0, inplace=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): res_block(\n",
      "      (param_dict): ParameterDict()\n",
      "      (relu): ReLU()\n",
      "      (dropout1): Dropout(p=0, inplace=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(70, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout2): Dropout(p=0, inplace=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): res_block(\n",
      "      (param_dict): ParameterDict()\n",
      "      (relu): ReLU()\n",
      "      (dropout1): Dropout(p=0, inplace=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(70, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout2): Dropout(p=0, inplace=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): res_block(\n",
      "      (param_dict): ParameterDict()\n",
      "      (relu): ReLU()\n",
      "      (dropout1): Dropout(p=0, inplace=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(70, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout2): Dropout(p=0, inplace=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): res_block(\n",
      "      (param_dict): ParameterDict()\n",
      "      (relu): ReLU()\n",
      "      (dropout1): Dropout(p=0, inplace=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(134, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout2): Dropout(p=0, inplace=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): res_block(\n",
      "      (param_dict): ParameterDict()\n",
      "      (relu): ReLU()\n",
      "      (dropout1): Dropout(p=0, inplace=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(134, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout2): Dropout(p=0, inplace=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): adaptiveLinear()\n",
      "  )\n",
      ")\n",
      "posires.pt exists.\n",
      "epoch 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  train_loss:  0.1840391788774958  val_loss:  0.253755219604658  val_accuracy:  0.9205748350975814\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2  train_loss:  0.17193622715198076  val_loss:  0.4968288787033247  val_accuracy:  0.8382482943083165\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXy0lEQVR4nO2da4zV5bXGn8XADMMd5D5Q7hdBK+BwMXhDc1prYsU2aP1gOWlzaFpN2qQfTtPzQZN+MSenNv1w0gaPpvS0p9rUEkljQYK2WIPKSBEGUEBAGEBAkKszMJd1Psw2mdp5nzWdPbP3pO/zS8gM+5m19zv//37mv/de71rL3B1CiH9++pV7AUKI0iCzC5EJMrsQmSCzC5EJMrsQmdC/lA9WVVXlgwYNSupMA4DGxsZuP/aAAQOoXlFRQfWqqqqkdvHiRRrb2tpK9erqaqq3tbVRfeDAgUnt8uXLNDb6vc2M6levXqU6O+79+/OnX3TOmpqaqM7OWXROmpubqd6bx6WlpYXGsvN94cIFNDY2drq4osxuZvcA+CmACgD/4+5Psp8fNGgQ7rzzzqS+cOFC+nj19fVJLUohTp48merDhw+n+vTp05Papk2baGz0x2D+/PlUj544s2fPTmpbt26lsaNGjaJ69Mfg8OHDVJ84cWJSGzFiBI2dMGEC1d977z2qz5gxI6lF5+TUqVNUj/5QHThwgOrsdzt79iyNnTNnTlL71a9+ldS6/TLezCoA/DeALwGYB+BhM5vX3fsTQvQuxbxnXwLgoLsfcvdrAJ4DcH/PLEsI0dMUY/YaAMc6/L+hcNvfYGZrzKzOzOqil6NCiN6jGLN39iHA371xdve17l7r7rXsAxMhRO9SjNkbAHT81GsSgBPFLUcI0VsUY/btAGaZ2TQzqwTwNQAbemZZQoieptupN3dvMbPHAGxCe+rtWXffw2Kam5tx5syZpL579276mCw9FqXWWAoIiPOmgwcPTmos9QXwlCHAU0QAMGzYMKo/88wzSS1KrUU5/OixZ86cSXWWrz569CiNnTRpEtXvu+8+qq9fvz6pXXfddTQ2OqfR508sFw4A8+alE1dvvPEGjY3SoSmKyrO7+0sAXirmPoQQpUHbZYXIBJldiEyQ2YXIBJldiEyQ2YXIBJldiEwoaT17ZWUlLe2LcsIs73rbbbfR2IaGBqpH+WRWKjp+/Hgay0oSAeB3v/sd1a9du0b1nTt3JrVo/8Att9xC9Q8//JDqH3/8MdUXL16c1FasWEFjoxx+tP369ttvT2pRLvvcuXNUj54v0XN5//79SW3q1Kk0lvmA9UbQlV2ITJDZhcgEmV2ITJDZhcgEmV2ITJDZhciEkqbeKioqwo6ijOXLlye1Y8eOJTUgLmmM0lssDXTDDTfQ2Ndff53qN910E9WjTqV33HFHUrt06RKNjc7HjTfeSPWoDfa0adOSWnROoi6rf/zjH6nOUliPP/44jd2wgbdmiNpcR6k7lgoeMmQIjWXl2mxdurILkQkyuxCZILMLkQkyuxCZILMLkQkyuxCZILMLkQklz7Oz0r/Kykoaz0o5o3xvVGbarx//u8faHkeTUseMGUP1qIw0Grs8d+7cpLZs2TIa++6771I9yhdHZajbt29PaosWLaKxUQlrNOV13bp1Se3KlSs09uTJk1SPzknUupyNZS7mucj2i+jKLkQmyOxCZILMLkQmyOxCZILMLkQmyOxCZILMLkQmlDzPzmp1ozG3ra2tSW3QoEE0Nqo/Pn36NNUPHTqU1KK8aP/+/DBH7Z6jXDZrZR210I6OeTRWeenSpVRn+ezovqNa+9WrV1N9+vTpSS3aXxA9n6L9ByyPDgCjR49OamzMNQDs3bs3qTU1NSW1osxuZkcAXALQCqDF3WuLuT8hRO/RE1f2Fe7+UQ/cjxCiF9F7diEyoVizO4CXzextM1vT2Q+Y2RozqzOzumg/shCi9yj2Zfxydz9hZmMBbDazd939b6pC3H0tgLUAUFNT40U+nhCimxR1ZXf3E4WvpwGsB7CkJxYlhOh5um12MxtsZkM//R7AFwDU99TChBA9SzEv48cBWF/IEfcH8H/uvpEFtLS00PxkVEM8duzYpBb1II9qnw8fPkx1lneNauk/+eQTqkd9wqNc+JEjR5La1atXaeyXv/xlqkc151G/fraHYOTIkTQ2Gl0c9QGYMWNGUov66UdzBKJzEsWzPSPRHAKWZ6+oqEhq3Ta7ux8CwKcbCCH6DEq9CZEJMrsQmSCzC5EJMrsQmSCzC5EJJS1xra6uxuc///mkfuHCBRrPWk1HZYGbNm2iepQG2r9/f1IbOnQojR02bBjVWYtsgKccAeCWW25JalHq7eDBg1RnZaJAnIJiraTvuusuGstSigCwbds2qrPjFpWonjlzhupRqvfEiRNUv3jxYlKLSp5ZWo+l9HRlFyITZHYhMkFmFyITZHYhMkFmFyITZHYhMkFmFyITSppnB3gecMeOHTSWjSZua2ujsWxUNBCXoX79619PasePH6exzz33HNVZKSYQt6p+6623ktrkyZNp7LRp06jOSiYBYNeuXVRnLZWj2KiVdJSHf+2115JaNEZ71qxZVK+t5Y2U2X4SANi9ezfVGWxPCWtbriu7EJkgswuRCTK7EJkgswuRCTK7EJkgswuRCTK7EJlg7qUb0jJs2DBftmxZUj979iyNnzhxYlKLasqjuuuqqiqqs7bGUQ4/yie/8sorVF+8eDHVhw8fntTOnz9PY6O1RW2uo3w0I2oFHdXSR2OV2XmJYmtqaqgejZtme0Kix3/zzTdpLBs//tRTT+HYsWOdFsTryi5EJsjsQmSCzC5EJsjsQmSCzC5EJsjsQmSCzC5EJpS0nn3IkCG0x3k0RvfKlStJLcrZRr3Xo7puVlvd2NhIY6M9AFHd9ssvv0z1pUuXUp0R9bTfs2cP1efMmUP1VatWJbUXXniBxv75z3+m+kMPPUR1NoY7GjUdHZeo/8GUKVOofvPNNye1rVu30tiox0CK8MpuZs+a2Wkzq+9w2ygz22xmBwpf+YQFIUTZ6crL+F8AuOczt/0AwBZ3nwVgS+H/Qog+TGh2d98K4LOzcu4HsK7w/ToAK3t2WUKInqa7H9CNc/eTAFD4mnxDbGZrzKzOzOqi9zlCiN6j1z+Nd/e17l7r7rVR8YEQovfortlPmdkEACh8Pd1zSxJC9AbdNfsGAKsL368G8GLPLEcI0VuEeXYz+w2AOwGMNrMGAI8DeBLAb83smwCOAkgnUzvQr18/VFdXJ/Uo98lqiJuamrodCwCnT/MXJwsXLkxqUZ48muW9ZMkSqt9+++1UZ3nXV199lcayHgFAPMc86kvPZomzHgFAXNf9ox/9iOoTJkxIatH+gOj5sHz5cqpHb1nZ7zZyJM9kDx48OKmx50Jodnd/OCHdHcUKIfoO2i4rRCbI7EJkgswuRCbI7EJkgswuRCaUtMS1urqajrKdNGkSjWcli9HI5qhldpQqYS2TP/74YxrL0o1AnDaMRjqfOHEiqc2fP5/GHjp0iOrf+MY3qF5Mu+fKykoa+53vfIfqW7Zsofof/vCHpPboo4/S2Pr6eqpH5zxKK5p12u0ZAH+eA7y9N0u96couRCbI7EJkgswuRCbI7EJkgswuRCbI7EJkgswuRCaUNM/e1NSEffv2JfUxY8bQeJZLf++992hsNHq4paWF6qyMNRrZvH37dqr/9a9/pXo0PviLX/xiUnv++edp7OXLl6m+bds2qrPxwQAwc+bMpLZy5UoaG+Wqo+fLihUrktptt91GY9m6AeDIkSNUj9qijxs3LqlFx5Q9n1i7dV3ZhcgEmV2ITJDZhcgEmV2ITJDZhcgEmV2ITJDZhciEkubZKysraevh4cOH03jWDjqqhY/qk6OWykyP8ugHDx6kelRTHo02ZvnoaA8Ay/cCwMWLF6leVVVFdVbvHj121KMg2jvx0ksvJbWoVXQ0hjsaJx0dt7lz5ya18+fP09i9e/cmNdYbQVd2ITJBZhciE2R2ITJBZhciE2R2ITJBZhciE2R2ITKhpHn2xsZGWs/ORuwCwJkzZ5Jaa2srjY3ynmy0MAAcP348qb311ls0tn9/fphra2upHvW8Z/sP+vXjf8+jnvRRPXt0XH/+858ntUWLFtHYqF791KlTVGf7H4rp6w4AS5cupfrQoUOpznrDR/sPDhw4kNSKyrOb2bNmdtrM6jvc9oSZHTeznYV/90b3I4QoL115Gf8LAPd0cvtP3H1B4V96q5IQok8Qmt3dtwI4V4K1CCF6kWI+oHvMzHYVXuaPTP2Qma0xszozq/vkk0+KeDghRDF01+w/AzADwAIAJwH8OPWD7r7W3WvdvTYaniiE6D26ZXZ3P+Xure7eBuBpAEt6dllCiJ6mW2Y3s445sgcA8PpRIUTZCfPsZvYbAHcCGG1mDQAeB3CnmS0A4ACOAPhWVx7s0qVLeOWVV5L6qlWraDzrp33hwgUae/3111M9ymW/+OKLSW3jxo00Nnr7wvq+dyWe5Wyj+ekDBw6k+sKFC6l+9uxZqi9btiypRT0EolkAUW/3efPmJTWWjwaAN954g+oLFiygerQ2Njs+qrWfPXt2UmN7D0Kzu/vDndz8TBQnhOhbaLusEJkgswuRCTK7EJkgswuRCTK7EJlQ0hLXtrY2sC2zUSkoa0MdlYmy0lqAtx0GeJtrluIB4rTfXXfdRXU2LhoAPvroo6T29NNP09gHHnigqMeO4lnLZZaGBeLnw8mTJ6k+evTopBaV5kajrKO0YGVlJdVZyvPb3/42jd20aVNSe+edd5KaruxCZILMLkQmyOxCZILMLkQmyOxCZILMLkQmyOxCZIJFpZ09ydChQ52VBt533300/tixY0lt5MhkZywAwObNm6kewUoiP/e5z9HYrVu3Un3+/PlU/8pXvkJ1dlyi0t9oNPHgwYOpHuWTWblmNGb75ptvpnoE+93ffvttGnvp0iWqz5o1i+qs7TnAn+vRqGp2Th577DHs37+/0z7YurILkQkyuxCZILMLkQkyuxCZILMLkQkyuxCZILMLkQklrWdvaWmhtddRe9877rgjqe3YsYPGRmNwx44dS/Xm5uakFtVd33333VSP8qpR7fQHH3yQ1KJc9rRp06ge1XWz8wnwmvMoFx3l8MePH0/1+vr0OIOoxXZDQwPVo1r6hx56iOq33nprUovGPbNafJaD15VdiEyQ2YXIBJldiEyQ2YXIBJldiEyQ2YXIBJldiEwoaZ59xIgR+OpXv5rUo9HELFdeUVFBY1m/egA4d+4c1VnN+tSpU2nslStXqH716lWqRz3OWd/5aCxytD8hGun84IMPUn3dunVJLepfcOTIEaqzPDrA9xhUV1fT2EWLFlGdjQ8H4rVNmTIlqUW19CwPz/aDhFd2M5tsZq+a2T4z22Nm3y3cPsrMNpvZgcJX3j1CCFFWuvIyvgXA9939egDLADxqZvMA/ADAFnefBWBL4f9CiD5KaHZ3P+nuOwrfXwKwD0ANgPsBfPoabR2Alb20RiFED/APfUBnZlMBLATwJoBx7n4SaP+DAKDTzeVmtsbM6sysLnrfLIToPbpsdjMbAuAFAN9zd/6JUQfcfa2717p7bfQBnBCi9+iS2c1sANqN/mt3/33h5lNmNqGgTwCQbiMqhCg7YerNzAzAMwD2uftTHaQNAFYDeLLw9cXovpqbm3HixImkvnLlShq/d+/epLZmzRoaG5UN/ulPf6I6S39FqbF+/fjf1A8//JDq165do/qNN96Y1N5//30ay9pQA0BNTQ3V6+rqqD5hwoSktnHjRhobldey0l6Aj+mOypKjFtqsJToQjxA/e/ZsUotStWx8OKMrefblAB4BsNvMdhZu+yHaTf5bM/smgKMAVnVrBUKIkhCa3d3/AqDTpvMAeFcGIUSfQdtlhcgEmV2ITJDZhcgEmV2ITJDZhciEkpa4NjU10bbIr7/+Oo0/f/58tzQgzidH+pAhQ5JaNJI5Gj0ctXuOthmzksgodvny5VR/5513qB7lum+66aak9u6779LYqGz5kUceoTo7L8OGDaOxUavpaO/E888/T/W5c+cmtZkzZ9JYdtzYmGpd2YXIBJldiEyQ2YXIBJldiEyQ2YXIBJldiEyQ2YXIhJLm2Zubm3H8+PGkvnnzZhrP2vuyOnkA2L9/P9Wj+uTDhw8ntdbWVhrb3hIgTZSzjcYDM1jeFYhr5aNx0tHI58bGxqQWtaGOegxs27aN6qzuO/q9onbOu3fvpno0bvrAgQNJLXo+sXNWVCtpIcQ/BzK7EJkgswuRCTK7EJkgswuRCTK7EJkgswuRCSXNs/fr14/24456nDc1NSW1MWPG0NiqqiqqR726x47tdLoVgDgvGuV0P/roI6pHuWy2/yC67+i4sP0FQDzqmp3vaG0LFy6k+qlTp3rtsaO9D9HI5+uuu47qo0ePTmqjRo2isUePHk1q7p7UdGUXIhNkdiEyQWYXIhNkdiEyQWYXIhNkdiEyQWYXIhO6Mp99MoBfAhgPoA3AWnf/qZk9AeDfAJwp/OgP3f0ldl8DBw7E9ddfn9TXr19P1zJ79uykxup4gbgHeTTzeurUqUmN5TYBYNeuXVSPap8HDhxI9dOnTye1+vp6GhvldK9evUp11k8f4PlotncBKL5mnM2Gj54PUR5+3Lhx3X5sABgwYEBSa2lpobHsmLP9AV3ZVNMC4PvuvsPMhgJ428w+7TLxE3f/ry7chxCizHRlPvtJACcL318ys30Aanp7YUKInuUfes9uZlMBLATwZuGmx8xsl5k9a2YjEzFrzKzOzOqil4RCiN6jy2Y3syEAXgDwPXe/COBnAGYAWID2K/+PO4tz97XuXuvutdE+bCFE79Els5vZALQb/dfu/nsAcPdT7t7q7m0AngawpPeWKYQoltDs1t4a9RkA+9z9qQ63d/y48QEA/GNfIURZ6cqn8csBPAJgt5ntLNz2QwAPm9kCAA7gCIBvhQ/Wvz9GjBiR1NesWUPj2fjhYstMz5w5Q/WDBw8mtWXLltHY/v35YY5Sd1GZ6WuvvZbUojLR6HOUPXv2UH3x4sXdvv+6ujoaO3/+fKpH46hZiWtENAI8SodOnDiR6ixdGqXeWJqZPZe68mn8XwB01vic5tSFEH0L7aATIhNkdiEyQWYXIhNkdiEyQWYXIhNkdiEyoaStpNva2mhulJX9AaA5+mPHjtHYmhpeu8NGCwPA5MmTk1qUi54zZw7Vo/HAUSnoyJGdliUAiEcyR/sTWMtjIB6Vzc5ZlKtuaGig+vjx46nOynujkuZ7772X6tHvHe2tYMc9apHNzgkrcdWVXYhMkNmFyASZXYhMkNmFyASZXYhMkNmFyASZXYhMsKiWukcfzOwMgA863DQaAO/ZWz766tr66roAra279OTaprh7p/PLS2r2v3twszp3ry3bAgh9dW19dV2A1tZdSrU2vYwXIhNkdiEyodxmX1vmx2f01bX11XUBWlt3KcnayvqeXQhROsp9ZRdClAiZXYhMKIvZzeweM3vPzA6a2Q/KsYYUZnbEzHab2U4z443Ne38tz5rZaTOr73DbKDPbbGYHCl/TxeylX9sTZna8cOx2mhkvCu+9tU02s1fNbJ+Z7TGz7xZuL+uxI+sqyXEr+Xt2M6sAsB/AvwBoALAdwMPuvrekC0lgZkcA1Lp72TdgmNntAC4D+KW731C47T8BnHP3Jwt/KEe6+7/3kbU9AeByucd4F6YVTeg4ZhzASgD/ijIeO7KuB1GC41aOK/sSAAfd/ZC7XwPwHID7y7COPo+7bwVw7jM33w9gXeH7dWh/spScxNr6BO5+0t13FL6/BODTMeNlPXZkXSWhHGavAdCxh1QD+ta8dwfwspm9bWZ8HlV5GOfuJ4H2Jw8A3rOq9IRjvEvJZ8aM95lj153x58VSDrN3NkqqL+X/lrv7IgBfAvBo4eWq6BpdGuNdKjoZM94n6O7482Iph9kbAHTs3jgJAO/eV0Lc/UTh62kA69H3RlGf+nSCbuFrekJgielLY7w7GzOOPnDsyjn+vBxm3w5glplNM7NKAF8DsKEM6/g7zGxw4YMTmNlgAF9A3xtFvQHA6sL3qwG8WMa1/A19ZYx3asw4ynzsyj7+3N1L/g/AvWj/RP59AP9RjjUk1jUdwDuFf3vKvTYAv0H7y7pmtL8i+iaA6wBsAXCg8HVUH1rb/wLYDWAX2o01oUxruxXtbw13AdhZ+HdvuY8dWVdJjpu2ywqRCdpBJ0QmyOxCZILMLkQmyOxCZILMLkQmyOxCZILMLkQm/D+GOhF7OucXEQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m val_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m accuracy_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loss_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(optimizer, model, loss_fn, train_loader, test_loader, epochs, train_loss_list, val_loss_list, accuracy_list)\u001b[0m\n\u001b[1;32m     30\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 32\u001b[0m     loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m loss_train\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     34\u001b[0m loss_validation, accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, loss_fn)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = posiresNN()\n",
    "print(model)\n",
    "filename_parameters = model.modelname+'.pt'\n",
    "dummy_tensor = torch.rand((1,3,28,28), requires_grad = False).to(device = device)\n",
    "model(dummy_tensor)\n",
    "if exists(filename_parameters):\n",
    "    print(filename_parameters, \"exists.\")\n",
    "\n",
    "    model.load_state_dict(torch.load(\"./\" + filename_parameters))\n",
    "else:\n",
    "    print(filename_parameters, \"does not exists.\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = 0.004)\n",
    "train_loss_list =[]\n",
    "val_loss_list = []\n",
    "accuracy_list = []\n",
    "training_loop(optimizer, model, loss_fn, train_loader, test_loader, EPOCHES, train_loss_list, val_loss_list, accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(evaluate_model(model, test_loader, loss_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = None\n",
    "for items in train_loader:\n",
    "    sample = items\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46adfd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118181ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47394f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc34c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num =1430\n",
    "sample_image = ut(sample[0][num].unsqueeze(0)).squeeze(0)\n",
    "plt.imshow(sample_image.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcc29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 6\n",
    "model.residual[layer].clearbackup()\n",
    "print(model(sample_image.unsqueeze(0)))\n",
    "insight = model.residual[layer].outlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093b2c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "insight[0].permute(0,2,3,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6984d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(tensor):\n",
    "    return tensor.to(device='cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a424eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in insight[2][0]:\n",
    "    plt.imshow(detach(image),cmap='gray')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35fc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(p1[0][2].to(device='cpu').detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60ff5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
